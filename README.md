# Text Emotion Recognition
Using Information Gain to decide which words are keywords to determine the emotion of text. 
There's an application of comments classification.
A.算法特点：


1.以信息增益大小为标准提取文本特征


【背景】


【参考网址：http://hxraid.iteye.com/blog/767364】


(1) 信息量是如何度量的 —— 信息熵 Entropy


因此先回忆一下信息论中有关信熵 的定义。说有这么一个变量X，它可能的取值有n多种，
分别是x1 ，x2 ，……，xn ，每一种取到的概率分别是P1 ，P2 ，……，Pn ，那么X的熵就
定义为：\

	H(x) = -SUM(P(i)*log2(P(i)))


(2) 在分类领域中，信息熵和信息增益的使用
对分类系统来说，类别C是变量，它可能的取值是C1 ，C2 ，……，Cn ，而每一个类别出现
的概率是P(C1 )，P(C2 )，……，P(Cn )，因此n就是类别的总数。此时整个分类系统的熵
就可以表示为：
	H(x) = -SUM(P(Ci)*log2(P(Ci)))

P(Ci)表示Ci类中包含文档数量占整个分类体系中全部文档数量的比重

(3)我们在分类系统中讨论特征t固定后的条件熵为： 
	H(C|T)=P(t)xH(C|t)+P(!t)xH(C|!t)
其中!t表示T中不包含t的条件。

(4)因此特征t的信息增益公式为：    
	IG(T) = H(C) - H(C|T)

2.利用神经网络构建文本情感分类模型
【背景】
【参考网址：http://blog.csdn.net/zhouchengyunew/article/details/6267193】
 1.神经网络结构：
 输入层 + 隐藏层(1或n层) + 输出层
 每层拥有多个节点，本次模型采用的节点数和特征的维数直接相关。
 每层与下一层神经节点采用全连接的方式。连接存在权重weight[i][j][k]
 表示第i层到第i+1层神经连接中第j个神经到第下一层第k个神经的连接权重。
 初始化采用0-1随机数的方式，但是本例中进行了归一化（原因是输入太稀疏，导致
 神经连接通过sigmoid函数后输出很容易走向极值1.0，因此尽量使得权重小。
 每个神经元Neuron，结构都如下：
 \w1
  \
   \__________________
___|sigmoid(sum(wi*x))|_____ y
w2 |__________________|
   /
  /
 /w3
 
[sigmoid(x)=1.0/(1.0+exp(-x))]

y = sigmoid(w1*x1+w2*x2+w3*x3+...)

【上述为前向神经网络传播过程，下面为反向误差传播以及对应的权值调整过程】

3.梯度下降训练：
误差公式：
	error = 1/2*sum(Yi-Y_expected(i))^2

采用链式法则进行从后往前的逐层对误差求导，得到如下公式：
	U(k) = SUM(W[i][j]X[k-1][j])
【本项目中采用无偏差模型，不存在阈值偏置BIAS】
X(k)为第k层输出：
	X(k) = sigmoid(U(k))
输出层的误差调节项(假设有m层神经)：
	D(m) = X(m)(1-X(m))(X(m)-Y) (Y是输出反馈)
以后逐层误差调整项由下面公式计算：
	D(k) = X(k)(1-X(k))SUM(D(k+1).*W[k])
而权重的调整公式如下：(第i层调整)(eta为学习率)
	W[i](t+1) = W[i](t) - eta*D(k)*X(k-1)

 
B.算法实现[Python Code]：
项目准备：1.安装Python科学计算库：numpy【提供更高效的矩阵计算方式】
	  2.准备好分词后的评价文本集合【见附件中的comments文件夹】
第一部分：3.读取原始分词文本集合，构建列表，统计每个出现过的词语对应的
	    文档是好评或差评的个数，并统计所有好评个数以及差评个数。
	  4.基于上面得到的统计信息对每个出现过的词语terms进行信息增益
	    计算，然后进行筛选，实验发现当选取的最小信息增益minIG=0.001
	    时，所得的筛选后的词汇表个数(大约1千)较为合适，适合作为特征
	    向量进行模式训练。
	  5.将得到的特征词汇写入文件中（参见附件中features.txt），至此，
	    基于信息增益的文本特征提取工作就全部结束了。
第二部分：6.创建反向传播神经网络BPNN的类的通用实现。其实现代码参见附件中
	    bpnn.py文件。
	  7.创建模型训练实例，选取一层节点个数为输入层的1/2的隐藏层，输出
	    层节点个数为1，并且设定模型学习率为0.3，学习次数为50000。
	  8.初始化权重为0-1范围内的随机数并对所有层之间的连接矩阵w[i]归一化。
	  9.对输入的数据进行归一化处理，输入数据解释如下：
	    在评论中发现了这个词语则赋值0.9，没有发现赋值0.1（这样做是因为神经
	    网络中所用到的sigmoid函数对0和1这样的极端数据的不敏感性，因此采用
	    近似0和近似1的0.1和0.9进行代替。输出反馈也如此（0->0.1,1->0.9）。
第四部分：10.训练神经网络，同时观察终端输出的平均错误指标，以便于判断本次训练
	     的结果是否符合要求。实验中训练采用随机样本训练，5万次训练共耗费
	     时间为12分钟（机器配置：单核cpu+4G内存)，训练结束后，训练得到的
	     神经网络权重值矩阵都被保存到了weight.txt文件中。
	  11.对得到的BPNN分类预测模型进行测试，选取样本中未被选择为训练数据的
	     分词后的评价集合（占总样本集合的20%）作为本次测试的测试数据集，
	     同时按照9中，对每条评论进行特征统计，得到0-1组成的大约1000维度的
	     向量，这个向量就是我们要进行分类识别的模式。将该向量转化为0.1-0.9
	     的向量，如同步骤9.将测试模式依次输入从权重矩阵文件weight.txt中
	     恢复得到的神经网络中，调用BPNN的apply函数仅仅进行网络的前向计算，
	     得到输出返回，然后比较这个输出和0.5，来判断最终的分类结果（0为
	     差评，1为好评）和我们已知的预期标签进行比较，统计最终正确率，整个
	     测试的输出结果可以在附件中test_output.txt中看到，正确率为90.5%。
	  12.至此整个模型和算法全部完成。若要对模型进行测试，请添加你要进行测试
	     评论到haopingfenci.txt和chapingfenci.txt文件中，然后再次运行:
	     python test_model.py 就能在终端上看到最终的正确率。
